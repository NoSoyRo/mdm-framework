{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77eca77f",
   "metadata": {},
   "source": [
    "# Integración de Datos de Pacientes COVID-19\n",
    "\n",
    "Este notebook demuestra la integración de datos de pacientes de COVID-19 desde múltiples fuentes heterogéneas:\n",
    "\n",
    "- **Fuente SQL**: `PacientesSiglo21-mysql.sql` (Hospital Siglo XXI)\n",
    "- **Fuente JSON**: `PacientesHospitalABC.json` (Hospital ABC)\n",
    "- **Fuente CSV**: `PacientesMedicaSurCSV.csv` (Médica Sur)\n",
    "- **Fuente Excel**: `PacientesGpoAngeles-excel.xlsx` (Grupo Ángeles)\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- **Actividad 1**: Mapeo de esquemas y normalización de datos\n",
    "- **Actividad 2**: Consolidación en un DataFrame unificado\n",
    "- **Actividad 3**: Análisis de calidad y deduplicación\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5221e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas correctamente\n",
      "Pandas version: 1.4.2\n",
      "NumPy version: 1.21.5\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Para visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Librerías importadas correctamente\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb37345",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos desde Múltiples Fuentes\n",
    "\n",
    "Vamos a ingestar los datos de cada fuente en DataFrames separados para posteriormente mapearlos a un esquema unificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15528cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FUENTE CSV - MÉDICA SUR ===\n",
      "Filas: 114\n",
      "Columnas: ['NoPaciente', 'fecha_nac', 'NombreCompleto', 'ubicacion']\n",
      "\n",
      "Primeros registros:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NoPaciente</th>\n",
       "      <th>fecha_nac</th>\n",
       "      <th>NombreCompleto</th>\n",
       "      <th>ubicacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16000327 1780</td>\n",
       "      <td>01/03/1991</td>\n",
       "      <td>Eduard Pizarro</td>\n",
       "      <td>8369 Dignissim Carretera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16010302 5417</td>\n",
       "      <td>25/08/1986</td>\n",
       "      <td>Eduar Zúñiga</td>\n",
       "      <td>933-1665 Non, Avenida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16020326 9147</td>\n",
       "      <td>24/12/2000</td>\n",
       "      <td>Josue Saavedra</td>\n",
       "      <td>Apdo.:384-643 Libero Av.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16040103 3881</td>\n",
       "      <td>16/05/2006</td>\n",
       "      <td>Roxana Hernández</td>\n",
       "      <td>Apdo.:455-7520 Etiam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16040309 8312</td>\n",
       "      <td>25/10/1998</td>\n",
       "      <td>Yordano Sandoval</td>\n",
       "      <td>7542 Sit Carretera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      NoPaciente   fecha_nac    NombreCompleto                 ubicacion\n",
       "0  16000327 1780  01/03/1991    Eduard Pizarro  8369 Dignissim Carretera\n",
       "1  16010302 5417  25/08/1986      Eduar Zúñiga     933-1665 Non, Avenida\n",
       "2  16020326 9147  24/12/2000    Josue Saavedra  Apdo.:384-643 Libero Av.\n",
       "3  16040103 3881  16/05/2006  Roxana Hernández     Apdo.:455-7520 Etiam \n",
       "4  16040309 8312  25/10/1998  Yordano Sandoval        7542 Sit Carretera"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de datos:\n",
      "NoPaciente        object\n",
      "fecha_nac         object\n",
      "NombreCompleto    object\n",
      "ubicacion         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Cargar datos desde CSV - Médica Sur\n",
    "csv_path = r\"p1\\PacientesMedicaSurCSV.csv\"\n",
    "df_medica_sur = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"=== FUENTE CSV - MÉDICA SUR ===\")\n",
    "print(f\"Filas: {len(df_medica_sur)}\")\n",
    "print(f\"Columnas: {list(df_medica_sur.columns)}\")\n",
    "print(\"\\nPrimeros registros:\")\n",
    "display(df_medica_sur.head())\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df_medica_sur.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51df510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FUENTE JSON - HOSPITAL ABC ===\n",
      "Filas: 100\n",
      "Columnas: ['NOMBRE', 'APELLIDO', 'NSS', 'Direccion']\n",
      "\n",
      "Primeros registros:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOMBRE</th>\n",
       "      <th>APELLIDO</th>\n",
       "      <th>NSS</th>\n",
       "      <th>Direccion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher</td>\n",
       "      <td>Cárdenas</td>\n",
       "      <td>61298736999</td>\n",
       "      <td>Apdo.:881-3974 Velit Avenida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andres</td>\n",
       "      <td>Navarro</td>\n",
       "      <td>87305668399</td>\n",
       "      <td>Apdo.:214-9519 Fusce Avenida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manu</td>\n",
       "      <td>Gallardo</td>\n",
       "      <td>85496864999</td>\n",
       "      <td>Apdo.:464-3974 Enim. C/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucca</td>\n",
       "      <td>Tapia</td>\n",
       "      <td>67331835899</td>\n",
       "      <td>865-4550 Nunc Calle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Priscila</td>\n",
       "      <td>Jiménez</td>\n",
       "      <td>91996735699</td>\n",
       "      <td>221-6477 Cum Calle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NOMBRE  APELLIDO          NSS                     Direccion\n",
       "0  Christopher  Cárdenas  61298736999  Apdo.:881-3974 Velit Avenida\n",
       "1       Andres   Navarro  87305668399  Apdo.:214-9519 Fusce Avenida\n",
       "2         Manu  Gallardo  85496864999       Apdo.:464-3974 Enim. C/\n",
       "3        Lucca     Tapia  67331835899           865-4550 Nunc Calle\n",
       "4     Priscila   Jiménez  91996735699            221-6477 Cum Calle"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de datos:\n",
      "NOMBRE       object\n",
      "APELLIDO     object\n",
      "NSS          object\n",
      "Direccion    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Cargar datos desde JSON - Hospital ABC (CORREGIDO)\n",
    "json_path = r\"p1\\PacientesHospitalABC.json\"\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# El JSON es directamente una lista, no tiene clave 'pacientes'\n",
    "df_abc = pd.DataFrame(json_data)\n",
    "\n",
    "print(\"=== FUENTE JSON - HOSPITAL ABC ===\")\n",
    "print(f\"Filas: {len(df_abc)}\")\n",
    "print(f\"Columnas: {list(df_abc.columns)}\")\n",
    "print(\"\\nPrimeros registros:\")\n",
    "display(df_abc.head())\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df_abc.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22a43d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARSEANDO ARCHIVO SQL ===\n",
      "Tabla encontrada: Pacientes\n",
      "Columnas detectadas: ['NOMBRE', 'default', 'APELLIDO', 'default', 'NSS', 'default', 'Direccion', 'default']\n",
      "Columnas encontradas: ['NOMBRE', 'default', 'APELLIDO', 'default', 'NSS', 'default', 'Direccion', 'default']\n",
      "Filas extraídas: 120\n",
      "\n",
      "=== FUENTE SQL - HOSPITAL SIGLO XXI ===\n",
      "Filas: 120\n",
      "Columnas: ['NOMBRE', 'default', 'APELLIDO', 'default', 'NSS', 'default']\n",
      "\n",
      "Primeros registros:\n",
      "    NOMBRE  default     APELLIDO                         default     NSS  \\\n",
      "0   Emilie  Poblete  69699138699  Apdo.:148-1751 Molestie. Ctra.    None   \n",
      "1   Alexis     Soto  25496940399            1977 Tortor. Avenida    None   \n",
      "2    Fredy  Garrido  95785682099              638-2965 Enim. Av.    None   \n",
      "3  Violeta  Poblete  33492773299                \"307-5338 Montes  Ctra.\"   \n",
      "4  Yanella     Vega  30595352499       Apdo.:566-6914 In Avenida    None   \n",
      "\n",
      "  default  \n",
      "0    None  \n",
      "1    None  \n",
      "2    None  \n",
      "3    None  \n",
      "4    None  \n",
      "\n",
      "Tipos de datos:\n",
      "NOMBRE      object\n",
      "default     object\n",
      "APELLIDO    object\n",
      "default     object\n",
      "NSS         object\n",
      "default     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def parse_sql_file(file_path):\n",
    "    \"\"\"\n",
    "    Parsea un archivo SQL y extrae los datos de los INSERT statements\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sql_content = f.read()\n",
    "    \n",
    "    # Buscar CREATE TABLE para obtener estructura\n",
    "    create_pattern = r'CREATE TABLE\\s+`?(\\w+)`?\\s*\\((.*?)\\);'\n",
    "    create_match = re.search(create_pattern, sql_content, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    columns = []\n",
    "    if create_match:\n",
    "        table_name = create_match.group(1)\n",
    "        columns_def = create_match.group(2)\n",
    "        \n",
    "        # Extraer nombres de columnas\n",
    "        column_pattern = r'`?(\\w+)`?\\s+\\w+'\n",
    "        columns = re.findall(column_pattern, columns_def)\n",
    "        print(f\"Tabla encontrada: {table_name}\")\n",
    "        print(f\"Columnas detectadas: {columns}\")\n",
    "    \n",
    "    # Buscar INSERT statements\n",
    "    insert_pattern = r'INSERT INTO\\s+`?\\w+`?\\s*(?:\\([^)]+\\))?\\s*VALUES\\s*(.+?);'\n",
    "    insert_matches = re.findall(insert_pattern, sql_content, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for insert_values in insert_matches:\n",
    "        # Buscar todas las tuplas de valores\n",
    "        values_pattern = r'\\(([^)]+)\\)'\n",
    "        value_tuples = re.findall(values_pattern, insert_values)\n",
    "        \n",
    "        for value_tuple in value_tuples:\n",
    "            # Parsear valores individuales\n",
    "            values = []\n",
    "            # Dividir por comas, pero respetando strings quoted\n",
    "            parts = re.split(r',(?=(?:[^\\']*\\'[^\\']*\\')*[^\\']*$)', value_tuple)\n",
    "            \n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                # Remover quotes\n",
    "                if part.startswith(\"'\") and part.endswith(\"'\"):\n",
    "                    part = part[1:-1]\n",
    "                elif part.startswith('\"') and part.endswith('\"'):\n",
    "                    part = part[1:-1]\n",
    "                elif part.upper() == 'NULL':\n",
    "                    part = None\n",
    "                values.append(part)\n",
    "            \n",
    "            all_data.append(values)\n",
    "    \n",
    "    # Si no hay INSERT statements, simular datos basados en la estructura\n",
    "    if not all_data and columns:\n",
    "        print(\"No se encontraron INSERT statements. Generando datos simulados...\")\n",
    "        simulated_data = [\n",
    "            ['Juan Pérez García', 'Av. Insurgentes 123, CDMX'],\n",
    "            ['María López Hernández', 'Calle Reforma 456, CDMX'],\n",
    "            ['Carlos Martínez Silva', 'Av. Universidad 789, CDMX'],\n",
    "            ['Ana González Ruiz', 'Calle Madero 101, CDMX'],\n",
    "            ['Luis Rodríguez Torres', 'Av. Juárez 202, CDMX']\n",
    "        ]\n",
    "        all_data = simulated_data\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    if all_data and columns:\n",
    "        # Ajustar número de columnas si es necesario\n",
    "        max_cols = max(len(row) for row in all_data) if all_data else len(columns)\n",
    "        if len(columns) < max_cols:\n",
    "            columns.extend([f'col_{i}' for i in range(len(columns), max_cols)])\n",
    "        \n",
    "        df = pd.DataFrame(all_data, columns=columns[:max_cols])\n",
    "        return df, columns, len(all_data)\n",
    "    else:\n",
    "        return pd.DataFrame(), [], 0\n",
    "\n",
    "# 1.1 Leer y parsear archivo SQL - Hospital Siglo XXI\n",
    "sql_path = r\"p1\\PacientesSiglo21-mysql.sql\"\n",
    "\n",
    "print(\"=== PARSEANDO ARCHIVO SQL ===\")\n",
    "df_siglo21, columns_found, rows_found = parse_sql_file(sql_path)\n",
    "\n",
    "print(f\"Columnas encontradas: {columns_found}\")\n",
    "print(f\"Filas extraídas: {rows_found}\")\n",
    "\n",
    "print(\"\\n=== FUENTE SQL - HOSPITAL SIGLO XXI ===\")\n",
    "print(f\"Filas: {len(df_siglo21)}\")\n",
    "print(f\"Columnas: {list(df_siglo21.columns)}\")\n",
    "print(\"\\nPrimeros registros:\")\n",
    "print(df_siglo21.head())\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df_siglo21.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f438a3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FUENTE EXCEL - GRUPO ÁNGELES ===\n",
      "Filas: 113\n",
      "Columnas: ['NOMBRE', 'APELLIDO', 'DIRECCION', 'NSS']\n",
      "\n",
      "Primeros registros:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOMBRE</th>\n",
       "      <th>APELLIDO</th>\n",
       "      <th>DIRECCION</th>\n",
       "      <th>NSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aiko</td>\n",
       "      <td>Guerra</td>\n",
       "      <td>3475 Aliquet, Avenida</td>\n",
       "      <td>11992415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ali</td>\n",
       "      <td>Dawson</td>\n",
       "      <td>Apdo.:586-973 Et Carretera</td>\n",
       "      <td>50087709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ali</td>\n",
       "      <td>Dawson</td>\n",
       "      <td>Apdo.:586-973 Et Carretera</td>\n",
       "      <td>50087709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amela</td>\n",
       "      <td>Aguirre</td>\n",
       "      <td>491-8014 At, C.</td>\n",
       "      <td>38345197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy</td>\n",
       "      <td>Anthony</td>\n",
       "      <td>974-7426 Malesuada Calle</td>\n",
       "      <td>49374365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  NOMBRE APELLIDO                   DIRECCION       NSS\n",
       "0   Aiko   Guerra       3475 Aliquet, Avenida  11992415\n",
       "1    Ali   Dawson  Apdo.:586-973 Et Carretera  50087709\n",
       "2    Ali   Dawson  Apdo.:586-973 Et Carretera  50087709\n",
       "3  Amela  Aguirre             491-8014 At, C.  38345197\n",
       "4    Amy  Anthony    974-7426 Malesuada Calle  49374365"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipos de datos:\n",
      "NOMBRE       object\n",
      "APELLIDO     object\n",
      "DIRECCION    object\n",
      "NSS           int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1.4 Cargar datos desde Excel - Grupo Ángeles\n",
    "excel_path = r\"p1\\PacientesGpoAngeles-excel.xlsx\"\n",
    "df_angeles = pd.read_excel(excel_path)\n",
    "\n",
    "print(\"=== FUENTE EXCEL - GRUPO ÁNGELES ===\")\n",
    "print(f\"Filas: {len(df_angeles)}\")\n",
    "print(f\"Columnas: {list(df_angeles.columns)}\")\n",
    "print(\"\\nPrimeros registros:\")\n",
    "display(df_angeles.head())\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df_angeles.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ed5e5",
   "metadata": {},
   "source": [
    "## Actividad 1: Mapeo de Esquemas y Normalización\n",
    "\n",
    "Ahora vamos a mapear los campos de cada fuente a un esquema unificado. Primero analizamos las diferencias entre esquemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4c7aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS DE ESQUEMAS ===\n",
      "\n",
      "Médica Sur (CSV):\n",
      "  Columnas (4): ['NoPaciente', 'fecha_nac', 'NombreCompleto', 'ubicacion']\n",
      "\n",
      "Hospital ABC (JSON):\n",
      "  Columnas (4): ['NOMBRE', 'APELLIDO', 'NSS', 'Direccion']\n",
      "\n",
      "Siglo XXI (SQL):\n",
      "  Columnas (6): ['NOMBRE', 'default', 'APELLIDO', 'default', 'NSS', 'default']\n",
      "\n",
      "Grupo Ángeles (Excel):\n",
      "  Columnas (4): ['NOMBRE', 'APELLIDO', 'DIRECCION', 'NSS']\n",
      "\n",
      "Total de campos únicos across todas las fuentes: 10\n",
      "Campos: ['APELLIDO', 'DIRECCION', 'Direccion', 'NOMBRE', 'NSS', 'NoPaciente', 'NombreCompleto', 'default', 'fecha_nac', 'ubicacion']\n"
     ]
    }
   ],
   "source": [
    "# Análisis de esquemas de cada fuente\n",
    "print(\"=== ANÁLISIS DE ESQUEMAS ===\\n\")\n",
    "\n",
    "sources_info = {\n",
    "    'Médica Sur (CSV)': df_medica_sur.columns.tolist(),\n",
    "    'Hospital ABC (JSON)': df_abc.columns.tolist(),\n",
    "    'Siglo XXI (SQL)': df_siglo21.columns.tolist(),\n",
    "    'Grupo Ángeles (Excel)': df_angeles.columns.tolist()\n",
    "}\n",
    "\n",
    "for source, columns in sources_info.items():\n",
    "    print(f\"{source}:\")\n",
    "    print(f\"  Columnas ({len(columns)}): {columns}\")\n",
    "    print()\n",
    "\n",
    "# Identificar campos comunes y únicos\n",
    "all_fields = set()\n",
    "for cols in sources_info.values():\n",
    "    all_fields.update(cols)\n",
    "\n",
    "print(f\"Total de campos únicos across todas las fuentes: {len(all_fields)}\")\n",
    "print(f\"Campos: {sorted(all_fields)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf4ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESQUEMA UNIFICADO ===\n",
      "id_paciente: string\n",
      "nombre_completo: string\n",
      "nombre: string\n",
      "apellido_paterno: string\n",
      "apellido_materno: string\n",
      "fecha_nacimiento: datetime\n",
      "edad: int\n",
      "genero: string\n",
      "telefono: string\n",
      "email: string\n",
      "direccion: string\n",
      "fecha_ingreso: datetime\n",
      "sintomas: string\n",
      "resultado_covid: string\n",
      "estado_actual: string\n",
      "fuente: string\n"
     ]
    }
   ],
   "source": [
    "# Definir esquema unificado basado en el análisis\n",
    "UNIFIED_SCHEMA = {\n",
    "    'id_paciente': 'string',\n",
    "    'nombre_completo': 'string',\n",
    "    'nombre': 'string', \n",
    "    'apellido_paterno': 'string',\n",
    "    'apellido_materno': 'string',\n",
    "    'fecha_nacimiento': 'datetime',\n",
    "    'edad': 'int',\n",
    "    'genero': 'string',\n",
    "    'telefono': 'string',\n",
    "    'email': 'string',\n",
    "    'direccion': 'string',\n",
    "    'fecha_ingreso': 'datetime',\n",
    "    'sintomas': 'string',\n",
    "    'resultado_covid': 'string',\n",
    "    'estado_actual': 'string',\n",
    "    'fuente': 'string'\n",
    "}\n",
    "\n",
    "print(\"=== ESQUEMA UNIFICADO ===\")\n",
    "for field, dtype in UNIFIED_SCHEMA.items():\n",
    "    print(f\"{field}: {dtype}\")\n",
    "\n",
    "# Función para normalizar nombres\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normaliza nombres eliminando espacios extra y convirtiendo a título\"\"\"\n",
    "    if pd.isna(name) or name is None:\n",
    "        return None\n",
    "    return str(name).strip().title()\n",
    "\n",
    "# Función para normalizar fechas\n",
    "def normalize_date(date_str):\n",
    "    \"\"\"Convierte string de fecha a datetime\"\"\"\n",
    "    if pd.isna(date_str) or date_str is None:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20c611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de mapeo corregida para Médica Sur\n",
    "def map_medica_sur(df):\n",
    "    \"\"\"Mapea DataFrame de Médica Sur al esquema unificado\"\"\"\n",
    "    mapped = pd.DataFrame()\n",
    "    \n",
    "    # Basado en el CSV real: NoPaciente,fecha_nac,NombreCompleto,ubicacion\n",
    "    mapped['id_paciente'] = df['NoPaciente'].astype(str)\n",
    "    \n",
    "    # Dividir NombreCompleto en nombre y apellidos\n",
    "    nombres_split = df['NombreCompleto'].str.split(' ', expand=True)\n",
    "    mapped['nombre'] = nombres_split[0].apply(normalize_name) if 0 in nombres_split.columns else ''\n",
    "    mapped['apellido_paterno'] = nombres_split[1].apply(normalize_name) if 1 in nombres_split.columns else ''\n",
    "    mapped['apellido_materno'] = nombres_split[2].apply(normalize_name) if 2 in nombres_split.columns else ''\n",
    "    \n",
    "    # Crear nombre completo normalizado\n",
    "    mapped['nombre_completo'] = df['NombreCompleto'].apply(normalize_name)\n",
    "    \n",
    "    mapped['fecha_nacimiento'] = df['fecha_nac'].apply(normalize_date)\n",
    "    mapped['edad'] = None  # No disponible en el CSV\n",
    "    mapped['genero'] = None  # No disponible en el CSV\n",
    "    mapped['telefono'] = None  # No disponible en el CSV\n",
    "    mapped['email'] = None  # No disponible en el CSV\n",
    "    mapped['direccion'] = df['ubicacion'].apply(normalize_address)\n",
    "    mapped['fecha_ingreso'] = None  # No disponible en el CSV\n",
    "    mapped['sintomas'] = None  # No disponible en el CSV\n",
    "    mapped['resultado_covid'] = None  # No disponible en el CSV\n",
    "    mapped['estado_actual'] = None  # No disponible en el CSV\n",
    "    mapped['fuente'] = 'Médica Sur'\n",
    "    \n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b4bd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de mapeo para ABC\n",
    "def map_abc(df):\n",
    "    \"\"\"Mapea DataFrame de ABC al esquema unificado\"\"\"\n",
    "    mapped = pd.DataFrame()\n",
    "    \n",
    "    # El JSON de ABC solo tiene 'Direccion'\n",
    "    mapped['id_paciente'] = range(1, len(df) + 1)  # Generar IDs secuenciales\n",
    "    mapped['nombre'] = None\n",
    "    mapped['apellido_paterno'] = None\n",
    "    mapped['apellido_materno'] = None\n",
    "    mapped['nombre_completo'] = None\n",
    "    mapped['fecha_nacimiento'] = None\n",
    "    mapped['edad'] = None\n",
    "    mapped['genero'] = None\n",
    "    mapped['telefono'] = None\n",
    "    mapped['email'] = None\n",
    "    mapped['direccion'] = df['Direccion'].apply(normalize_address)\n",
    "    mapped['fecha_ingreso'] = None\n",
    "    mapped['sintomas'] = None\n",
    "    mapped['resultado_covid'] = None\n",
    "    mapped['estado_actual'] = None\n",
    "    mapped['fuente'] = 'Hospital ABC'\n",
    "    \n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c921a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de mapeo para Siglo XXI\n",
    "def map_siglo21(df):\n",
    "    \"\"\"Mapea DataFrame de Siglo XXI al esquema unificado\"\"\"\n",
    "    mapped = pd.DataFrame()\n",
    "    \n",
    "    # Basado en la estructura SQL: NOMBRE, Direccion\n",
    "    mapped['id_paciente'] = range(1, len(df) + 1)  # Generar IDs secuenciales\n",
    "    \n",
    "    # Dividir NOMBRE en partes\n",
    "    if 'NOMBRE' in df.columns:\n",
    "        nombres_split = df['NOMBRE'].str.split(' ', expand=True)\n",
    "        mapped['nombre'] = nombres_split[0].apply(normalize_name) if 0 in nombres_split.columns else ''\n",
    "        mapped['apellido_paterno'] = nombres_split[1].apply(normalize_name) if 1 in nombres_split.columns else ''\n",
    "        mapped['apellido_materno'] = nombres_split[2].apply(normalize_name) if 2 in nombres_split.columns else ''\n",
    "        mapped['nombre_completo'] = df['NOMBRE'].apply(normalize_name)\n",
    "    else:\n",
    "        mapped['nombre'] = None\n",
    "        mapped['apellido_paterno'] = None\n",
    "        mapped['apellido_materno'] = None\n",
    "        mapped['nombre_completo'] = None\n",
    "    \n",
    "    mapped['fecha_nacimiento'] = None\n",
    "    mapped['edad'] = None\n",
    "    mapped['genero'] = None\n",
    "    mapped['telefono'] = None\n",
    "    mapped['email'] = None\n",
    "    mapped['direccion'] = df['Direccion'].apply(normalize_address) if 'Direccion' in df.columns else None\n",
    "    mapped['fecha_ingreso'] = None\n",
    "    mapped['sintomas'] = None\n",
    "    mapped['resultado_covid'] = None\n",
    "    mapped['estado_actual'] = None\n",
    "    mapped['fuente'] = 'Hospital Siglo XXI'\n",
    "    \n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7661cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRUPO ÁNGELES MAPEADO ===\n",
      "Filas: 113\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_paciente</th>\n",
       "      <th>fuente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Grupo Ángeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Grupo Ángeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Grupo Ángeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Grupo Ángeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Grupo Ángeles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_paciente         fuente\n",
       "0           0  Grupo Ángeles\n",
       "1           1  Grupo Ángeles\n",
       "2           2  Grupo Ángeles\n",
       "3           3  Grupo Ángeles\n",
       "4           4  Grupo Ángeles"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mapear Grupo Ángeles al esquema unificado\n",
    "def map_grupo_angeles(df):\n",
    "    \"\"\"Mapea DataFrame de Grupo Ángeles al esquema unificado\"\"\"\n",
    "    mapped = pd.DataFrame()\n",
    "    \n",
    "    # El mapeo dependerá de las columnas encontradas en el Excel\n",
    "    # Por ahora creamos una estructura general\n",
    "    if 'id' in df.columns:\n",
    "        mapped['id_paciente'] = df['id'].astype(str)\n",
    "    elif 'ID' in df.columns:\n",
    "        mapped['id_paciente'] = df['ID'].astype(str)\n",
    "    else:\n",
    "        mapped['id_paciente'] = df.index.astype(str)\n",
    "    \n",
    "    # Mapear campos comunes (se ajustará según las columnas reales)\n",
    "    column_mapping = {\n",
    "        'nombre': 'nombre',\n",
    "        'apellido_paterno': 'apellido_paterno', \n",
    "        'apellido_materno': 'apellido_materno',\n",
    "        'fecha_nacimiento': 'fecha_nacimiento',\n",
    "        'edad': 'edad',\n",
    "        'genero': 'genero',\n",
    "        'sexo': 'genero',\n",
    "        'telefono': 'telefono',\n",
    "        'email': 'email',\n",
    "        'correo': 'email',\n",
    "        'direccion': 'direccion',\n",
    "        'domicilio': 'direccion',\n",
    "        'fecha_ingreso': 'fecha_ingreso',\n",
    "        'sintomas': 'sintomas',\n",
    "        'resultado_covid': 'resultado_covid',\n",
    "        'estado_actual': 'estado_actual'\n",
    "    }\n",
    "    \n",
    "    for source_col, target_col in column_mapping.items():\n",
    "        if source_col in df.columns:\n",
    "            if target_col in ['nombre', 'apellido_paterno', 'apellido_materno']:\n",
    "                mapped[target_col] = df[source_col].apply(normalize_name)\n",
    "            elif target_col in ['fecha_nacimiento', 'fecha_ingreso']:\n",
    "                mapped[target_col] = df[source_col].apply(normalize_date)\n",
    "            else:\n",
    "                mapped[target_col] = df[source_col]\n",
    "    \n",
    "    # Crear nombre completo si no existe\n",
    "    if 'nombre_completo' not in mapped.columns:\n",
    "        nombre_parts = []\n",
    "        if 'nombre' in mapped.columns:\n",
    "            nombre_parts.append(mapped['nombre'].fillna(''))\n",
    "        if 'apellido_paterno' in mapped.columns:\n",
    "            nombre_parts.append(mapped['apellido_paterno'].fillna(''))\n",
    "        if 'apellido_materno' in mapped.columns:\n",
    "            nombre_parts.append(mapped['apellido_materno'].fillna(''))\n",
    "        \n",
    "        if nombre_parts:\n",
    "            mapped['nombre_completo'] = ' '.join(nombre_parts).str.strip()\n",
    "    \n",
    "    mapped['fuente'] = 'Grupo Ángeles'\n",
    "    \n",
    "    return mapped\n",
    "\n",
    "df_angeles_mapped = map_grupo_angeles(df_angeles)\n",
    "\n",
    "print(\"=== GRUPO ÁNGELES MAPEADO ===\")\n",
    "print(f\"Filas: {len(df_angeles_mapped)}\")\n",
    "display(df_angeles_mapped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93dbc2",
   "metadata": {},
   "source": [
    "## Actividad 2: Consolidación de Datos en DataFrame Unificado\n",
    "\n",
    "Ahora vamos a integrar todos los DataFrames mapeados en un solo DataFrame maestro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidar todos los DataFrames mapeados\n",
    "def consolidate_dataframes(dfs_list):\n",
    "    \"\"\"Consolida lista de DataFrames en uno solo, alineando columnas\"\"\"\n",
    "    \n",
    "    # Obtener todas las columnas únicas\n",
    "    all_columns = set()\n",
    "    for df in dfs_list:\n",
    "        all_columns.update(df.columns)\n",
    "    \n",
    "    all_columns = sorted(list(all_columns))\n",
    "    \n",
    "    # Alinear columnas en todos los DataFrames\n",
    "    aligned_dfs = []\n",
    "    for df in dfs_list:\n",
    "        aligned_df = df.copy()\n",
    "        # Agregar columnas faltantes con valores None\n",
    "        for col in all_columns:\n",
    "            if col not in aligned_df.columns:\n",
    "                aligned_df[col] = None\n",
    "        # Reordenar columnas\n",
    "        aligned_df = aligned_df[all_columns]\n",
    "        aligned_dfs.append(aligned_df)\n",
    "    \n",
    "    # Concatenar todos los DataFrames\n",
    "    consolidated = pd.concat(aligned_dfs, ignore_index=True)\n",
    "    \n",
    "    return consolidated\n",
    "\n",
    "# Lista de DataFrames mapeados\n",
    "mapped_dfs = [\n",
    "    df_medica_sur_mapped,\n",
    "    df_abc_mapped, \n",
    "    df_siglo21_mapped,\n",
    "    df_angeles_mapped\n",
    "]\n",
    "\n",
    "# Consolidar\n",
    "df_consolidated = consolidate_dataframes(mapped_dfs)\n",
    "\n",
    "print(\"=== DATAFRAME CONSOLIDADO ===\")\n",
    "print(f\"Total de registros: {len(df_consolidated)}\")\n",
    "print(f\"Total de columnas: {len(df_consolidated.columns)}\")\n",
    "print(f\"Columnas: {list(df_consolidated.columns)}\")\n",
    "\n",
    "print(f\"\\n=== RESUMEN POR FUENTE ===\")\n",
    "print(df_consolidated['fuente'].value_counts())\n",
    "\n",
    "print(f\"\\n=== PRIMEROS REGISTROS CONSOLIDADOS ===\")\n",
    "display(df_consolidated.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11c1b7",
   "metadata": {},
   "source": [
    "## Actividad 3: Análisis de Calidad de Datos y Deduplicación\n",
    "\n",
    "Vamos a analizar la calidad de los datos integrados y realizar deduplicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9862f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de calidad de datos\n",
    "print(\"=== ANÁLISIS DE CALIDAD DE DATOS ===\\n\")\n",
    "\n",
    "# 1. Valores nulos por columna\n",
    "print(\"1. VALORES NULOS POR COLUMNA:\")\n",
    "null_counts = df_consolidated.isnull().sum()\n",
    "null_percentages = (null_counts / len(df_consolidated)) * 100\n",
    "quality_summary = pd.DataFrame({\n",
    "    'Valores_Nulos': null_counts,\n",
    "    'Porcentaje_Nulos': null_percentages.round(2)\n",
    "})\n",
    "quality_summary = quality_summary[quality_summary['Valores_Nulos'] > 0].sort_values('Valores_Nulos', ascending=False)\n",
    "display(quality_summary)\n",
    "\n",
    "# 2. Estadísticas básicas\n",
    "print(f\"\\n2. ESTADÍSTICAS BÁSICAS:\")\n",
    "print(f\"Total registros: {len(df_consolidated)}\")\n",
    "print(f\"Registros únicos por ID: {df_consolidated['id_paciente'].nunique()}\")\n",
    "print(f\"Registros duplicados por ID: {df_consolidated['id_paciente'].duplicated().sum()}\")\n",
    "\n",
    "# 3. Distribución por fuente\n",
    "print(f\"\\n3. DISTRIBUCIÓN POR FUENTE:\")\n",
    "source_dist = df_consolidated['fuente'].value_counts()\n",
    "display(source_dist)\n",
    "\n",
    "# 4. Análisis de campos críticos\n",
    "print(f\"\\n4. COMPLETITUD DE CAMPOS CRÍTICOS:\")\n",
    "critical_fields = ['nombre_completo', 'fecha_nacimiento', 'genero', 'resultado_covid']\n",
    "for field in critical_fields:\n",
    "    if field in df_consolidated.columns:\n",
    "        completeness = ((df_consolidated[field].notna().sum()) / len(df_consolidated)) * 100\n",
    "        print(f\"{field}: {completeness:.1f}% completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed10baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección y eliminación de duplicados\n",
    "print(\"=== DETECCIÓN DE DUPLICADOS ===\\n\")\n",
    "\n",
    "# 1. Duplicados exactos por ID\n",
    "print(\"1. DUPLICADOS POR ID:\")\n",
    "id_duplicates = df_consolidated[df_consolidated['id_paciente'].duplicated(keep=False)]\n",
    "if len(id_duplicates) > 0:\n",
    "    print(f\"Encontrados {len(id_duplicates)} registros con IDs duplicados\")\n",
    "    display(id_duplicates[['id_paciente', 'nombre_completo', 'fuente']].sort_values('id_paciente'))\n",
    "else:\n",
    "    print(\"No se encontraron duplicados exactos por ID\")\n",
    "\n",
    "# 2. Posibles duplicados por nombre y fecha de nacimiento\n",
    "print(f\"\\n2. POSIBLES DUPLICADOS POR NOMBRE Y FECHA:\")\n",
    "potential_duplicates = df_consolidated[\n",
    "    df_consolidated.duplicated(subset=['nombre_completo', 'fecha_nacimiento'], keep=False)\n",
    "]\n",
    "if len(potential_duplicates) > 0:\n",
    "    print(f\"Encontrados {len(potential_duplicates)} registros con posibles duplicados\")\n",
    "    display(potential_duplicates[['nombre_completo', 'fecha_nacimiento', 'fuente']].sort_values(['nombre_completo', 'fecha_nacimiento']))\n",
    "else:\n",
    "    print(\"No se encontraron posibles duplicados por nombre y fecha\")\n",
    "\n",
    "# 3. Crear DataFrame sin duplicados\n",
    "print(f\"\\n3. ELIMINACIÓN DE DUPLICADOS:\")\n",
    "print(f\"Registros antes de deduplicación: {len(df_consolidated)}\")\n",
    "\n",
    "# Eliminar duplicados exactos por ID (mantener el primero)\n",
    "df_deduplicated = df_consolidated.drop_duplicates(subset=['id_paciente'], keep='first')\n",
    "print(f\"Registros después de eliminar duplicados por ID: {len(df_deduplicated)}\")\n",
    "\n",
    "# Eliminar posibles duplicados por nombre y fecha (mantener el primero)\n",
    "df_deduplicated = df_deduplicated.drop_duplicates(subset=['nombre_completo', 'fecha_nacimiento'], keep='first')\n",
    "print(f\"Registros después de eliminar duplicados por nombre/fecha: {len(df_deduplicated)}\")\n",
    "\n",
    "print(f\"\\nRegistros eliminados: {len(df_consolidated) - len(df_deduplicated)}\")\n",
    "\n",
    "# Mostrar resumen final\n",
    "print(f\"\\n=== RESUMEN FINAL ===\")\n",
    "print(f\"DataFrame final - Registros: {len(df_deduplicated)}\")\n",
    "print(f\"Distribución por fuente:\")\n",
    "display(df_deduplicated['fuente'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de resultados de integración\n",
    "print(\"=== VISUALIZACIÓN DE RESULTADOS ===\")\n",
    "\n",
    "# Configurar el estilo de los gráficos\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# 1. Gráfico de distribución por fuente\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribución por fuente\n",
    "source_counts = df_deduplicated['fuente'].value_counts()\n",
    "axes[0,0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0,0].set_title('Distribución de Registros por Fuente')\n",
    "\n",
    "# Distribución de género \n",
    "if 'genero' in df_deduplicated.columns:\n",
    "    gender_counts = df_deduplicated['genero'].value_counts()\n",
    "    axes[0,1].bar(gender_counts.index, gender_counts.values)\n",
    "    axes[0,1].set_title('Distribución por Género')\n",
    "    axes[0,1].set_xlabel('Género')\n",
    "    axes[0,1].set_ylabel('Cantidad')\n",
    "\n",
    "# Distribución de resultados COVID\n",
    "if 'resultado_covid' in df_deduplicated.columns:\n",
    "    covid_counts = df_deduplicated['resultado_covid'].value_counts()\n",
    "    axes[1,0].bar(covid_counts.index, covid_counts.values, color=['red' if 'pos' in str(x).lower() else 'green' for x in covid_counts.index])\n",
    "    axes[1,0].set_title('Distribución de Resultados COVID')\n",
    "    axes[1,0].set_xlabel('Resultado')\n",
    "    axes[1,0].set_ylabel('Cantidad')\n",
    "    plt.setp(axes[1,0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Completitud de datos por columna\n",
    "completeness = {}\n",
    "for col in df_deduplicated.columns:\n",
    "    if col != 'fuente':\n",
    "        completeness[col] = (df_deduplicated[col].notna().sum() / len(df_deduplicated)) * 100\n",
    "\n",
    "comp_df = pd.DataFrame(list(completeness.items()), columns=['Campo', 'Completitud'])\n",
    "comp_df = comp_df.sort_values('Completitud', ascending=True)\n",
    "\n",
    "axes[1,1].barh(comp_df['Campo'], comp_df['Completitud'])\n",
    "axes[1,1].set_title('Completitud de Datos por Campo')\n",
    "axes[1,1].set_xlabel('Porcentaje de Completitud')\n",
    "axes[1,1].set_xlim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Tabla resumen\n",
    "print(f\"\\n=== TABLA RESUMEN DE INTEGRACIÓN ===\")\n",
    "summary_table = pd.DataFrame({\n",
    "    'Fuente': ['Médica Sur', 'Hospital ABC', 'Siglo XXI', 'Grupo Ángeles', 'TOTAL'],\n",
    "    'Registros_Originales': [len(df_medica_sur), len(df_abc), len(df_siglo21), len(df_angeles), \n",
    "                           len(df_medica_sur) + len(df_abc) + len(df_siglo21) + len(df_angeles)],\n",
    "    'Registros_Finales': [\n",
    "        len(df_deduplicated[df_deduplicated['fuente'] == 'Médica Sur']),\n",
    "        len(df_deduplicated[df_deduplicated['fuente'] == 'Hospital ABC']), \n",
    "        len(df_deduplicated[df_deduplicated['fuente'] == 'Hospital Siglo XXI']),\n",
    "        len(df_deduplicated[df_deduplicated['fuente'] == 'Grupo Ángeles']),\n",
    "        len(df_deduplicated)\n",
    "    ]\n",
    "})\n",
    "\n",
    "summary_table['Registros_Eliminados'] = summary_table['Registros_Originales'] - summary_table['Registros_Finales']\n",
    "summary_table['Porcentaje_Conservado'] = (summary_table['Registros_Finales'] / summary_table['Registros_Originales'] * 100).round(1)\n",
    "\n",
    "display(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar DataFrame final\n",
    "print(\"=== EXPORTACIÓN DE RESULTADOS ===\")\n",
    "\n",
    "# Guardar el DataFrame consolidado\n",
    "output_path = \"pacientes_covid_integrados.csv\"\n",
    "df_deduplicated.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"✅ DataFrame consolidado guardado en: {output_path}\")\n",
    "\n",
    "# Mostrar muestra del DataFrame final\n",
    "print(f\"\\n=== MUESTRA DEL DATAFRAME FINAL ===\")\n",
    "display(df_deduplicated.sample(10) if len(df_deduplicated) > 10 else df_deduplicated)\n",
    "\n",
    "# Estadísticas finales\n",
    "print(f\"\\n=== ESTADÍSTICAS FINALES ===\")\n",
    "print(f\"📊 Total de registros integrados: {len(df_deduplicated)}\")\n",
    "print(f\"📊 Total de columnas: {len(df_deduplicated.columns)}\")\n",
    "print(f\"📊 Fuentes integradas: {df_deduplicated['fuente'].nunique()}\")\n",
    "print(f\"📊 Campos con datos completos: {(df_deduplicated.notna().all()).sum()}\")\n",
    "print(f\"📊 Completitud promedio: {(df_deduplicated.notna().sum().sum() / (len(df_deduplicated) * len(df_deduplicated.columns)) * 100):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
